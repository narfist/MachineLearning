{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpamDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/udacity/machine-learning/blob/master/projects/practice_projects/naive_bayes_tutorial/Naive_Bayes_tutorial.ipynb\n",
    "\n",
    "First we will rebuild the Algo Bag of Words in separate Steps to represent the words in the coloums with numbers. Reason: just numbers can handeld by an learning-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                        sms_message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table('smsspamcollection/SMSSpamCollection',\n",
    "                   sep='\\t', \n",
    "                   header=None, \n",
    "                   names=['label', 'sms_message'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n",
      "   label                                        sms_message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# map ham and spam to 0/1 to deal with it\n",
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert to lower case\n",
      "['Hello, how are you!', 'Win money, win from home.', 'Call me now.', 'Hello, Call hello you tomorrow?']\n",
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "print(\"Convert to lower case\")\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "print(documents)\n",
    "lower_case_documents = []\n",
    "for i in documents:\n",
    "    lower_case_documents.append(i.lower())\n",
    "print(lower_case_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove all punctation\n",
      "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove all punctation\")\n",
    "import string\n",
    "without_punktation = []\n",
    "for i in lower_case_documents:\n",
    "    without_punktation.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
    "print(without_punktation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into single words\n",
      "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n",
      "[Counter({'hello': 1, 'you': 1, 'how': 1, 'are': 1}),\n",
      " Counter({'win': 2, 'from': 1, 'money': 1, 'home': 1}),\n",
      " Counter({'now': 1, 'me': 1, 'call': 1}),\n",
      " Counter({'hello': 2, 'you': 1, 'tomorrow': 1, 'call': 1})]\n"
     ]
    }
   ],
   "source": [
    "print(\"Split into single words\")\n",
    "words = []\n",
    "for i in without_punktation:\n",
    "    words.append(i.split(' '))\n",
    "print(words)\n",
    "\n",
    "gezaehlte_Liste = []\n",
    "import pprint\n",
    "from collections import Counter\n",
    "for i in words:\n",
    "    counts = Counter(i)\n",
    "    gezaehlte_Liste.append(counts)\n",
    "pprint.pprint(gezaehlte_Liste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For now we use the ready implemented Bag of Words-Algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First deal with test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'call',\n",
       " 'from',\n",
       " 'hello',\n",
       " 'home',\n",
       " 'how',\n",
       " 'me',\n",
       " 'money',\n",
       " 'now',\n",
       " 'tomorrow',\n",
       " 'win',\n",
       " 'you']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform the Vector to a Matrix\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 10)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 3)\t2\n",
      "  (3, 9)\t1\n",
      "  (3, 11)\t1\n",
      "and put it in an Array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Transform the Vector to a Matrix\")\n",
    "print(count_vector.transform(documents))\n",
    "print(\"and put it in an Array\")\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Matrix is transformed into an Array. The frequency of every word ist represented by the count of the numbers. To make it easier to read wie transform it into an DataFrame. There we have headers with the single words.\n",
    "\n",
    "\"DataFrame([data, index, columns, dtype, copy])\tTwo-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix = pd.DataFrame(doc_array,\n",
    "                           columns = count_vector.get_feature_names()\n",
    "                           )\n",
    "freq_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Training and Testing Set\n",
    "Now we use the train_test_split from sklearn. First import the needed function from the framework and than use it to store the four data in separate matrices. (Train and test for message and the label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 5572\n",
      "Number of rows in the training set: 3733\n",
      "Number of rows in the test set: 1839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['sms_message'],\n",
    "                                                    df['label'],\n",
    "                                                    random_state=1,\n",
    "                                                    test_size = 0.33)\n",
    "#print(\"X-Train-Matrix:\"); print(X_train); print(\"X-Test-Matrix:\"); print(X_test)\n",
    "#print(\"Y-Train-Matrix:\"); print(Y_train); print(\"Y-Test-Matrix:\"); print(Y_test)\n",
    "print('Number of rows in the total set: {}' .format(df.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Bag of Words to the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bayes Theorem\n",
    "First starting with a simple example - calculation the probaility of getting a positiv diabetis testresult\n",
    "\n",
    "\n",
    "![title](https://raw.githubusercontent.com/udacity/machine-learning/756d918d21551d3c3668ec6ee680947b253d1214/projects/practice_projects/naive_bayes_tutorial/images/bayes_formula.png)\n",
    "\n",
    "P(A) is the prior probability of A occuring independantly. In our example this is P(D). This value is given to us.\n",
    "P(B) is the prior probability of B occuring independantly. In our example this is P(Pos).\n",
    "P(A|B) is the posterior probability that A occurs given B. In our example this is P(D|Pos). That is, the probability of an individual having diabetes, given that, that individual got a positive test result. This is the value that we are looking to calculate.\n",
    "P(B|A) is the likelihood probability of B occuring, given A. In our example this is P(Pos|D). This value is given to us.\n",
    "\n",
    "Putting our values into the formula for Bayes theorem we get:\n",
    "P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "The probability of getting a positive test result P(Pos) can be calulated using the Sensitivity and Specificity as follows:\n",
    "P(Pos) = [P(D) * Sensitivity] + [P(~D) * (1-Specificity))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a positive test result P(Pos) is: {} 0.10799999999999998\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Calculate probability of getting a positive test result, P(Pos)\n",
    "'''\n",
    "#Given facts:\n",
    "#P(D)\n",
    "p_diabetis = 0.01\n",
    "#P{~D}\n",
    "p_no_diabetis = 0.99\n",
    "#Sensitivity or P(Pos|D) - fact that test with positive result work fine\n",
    "p_pos_diabetis = 0.9\n",
    "#Specificity or P(Neg|~D) - fact that test with negative result work fine\n",
    "p_negativ_no_diabetis = 0.9\n",
    "\n",
    "p_pos = (p_diabetis * p_pos_diabetis) + (p_no_diabetis * (1 - p_negativ_no_diabetis))\n",
    "print('The probability of getting a positive test result P(Pos) is: {}', format(p_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an individual having diabetes, given that, that individual got a positive test result:\n",
    "P(D/Pos) = (P(D) * Sensitivity)) / P(Pos)\n",
    "\n",
    "The probability of an individual not having diabetes, given that, that individual got a positive test result:\n",
    "P(~D/Pos) = (P(~D) * (1-Specificity)) / P(Pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual having diabetes, given that\n",
      "that individual got a positive test result is: {} 0.08333333333333336\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the probability of an individual having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(D|Pos).\n",
    "\n",
    "The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "'''\n",
    "p_diabetis_pos = (p_diabetis * p_pos_diabetis) / p_pos\n",
    "print('Probability of an individual having diabetes, given that')\n",
    "print('that individual got a positive test result is: {}', format(p_diabetis_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual not having diabetes,\n",
      "given that that individual got a positive test result is: {} 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(~D|Pos).\n",
    "\n",
    "The formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)\n",
    "\n",
    "Note that P(Pos/~D) can be computed as 1 - P(Neg/~D). \n",
    "\n",
    "Therefore:\n",
    "P(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\n",
    "'''\n",
    "p_no_diabetis_pos = (p_no_diabetis * (1 - p_negativ_no_diabetis) / p_pos)\n",
    "print('Probability of an individual not having diabetes,')\n",
    "print('given that that individual got a positive test result is: {}', p_no_diabetis_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented Bayes theorem from scratch. Your analysis shows that even if you get a positive test result, there is only a 8.3% chance that you actually have diabetes and a 91.67% chance that you do not have diabetes. This is of course assuming that only 1% of the entire population has diabetes which of course is only an assumption.\n",
    "\n",
    "# Calculating Speech-Terms\n",
    "\n",
    "Let's say that we have two political parties' candidates, 'Jill Stein' of the Green Party and 'Gary Johnson' of the Libertarian Party and we have the probabilities of each of these candidates saying the words 'freedom', 'immigration' and 'environment' when they give a speech:\n",
    "\n",
    "Probability that Jill Stein says 'freedom': 0.1 ---------> P(F|J)\n",
    "Probability that Jill Stein says 'immigration': 0.1 -----> P(I|J)\n",
    "Probability that Jill Stein says 'environment': 0.8 -----> P(E|J)\n",
    "Probability that Gary Johnson says 'freedom': 0.7 -------> P(F|G)\n",
    "Probability that Gary Johnson says 'immigration': 0.2 ---> P(I|G)\n",
    "Probability that Gary Johnson says 'environment': 0.1 ---> P(E|G)\n",
    "\n",
    "And let us also assume that the probablility of Jill Stein giving a speech, P(J) is 0.5 and the same for Gary Johnson, P(G) = 0.5.\n",
    "\n",
    "![title](https://github.com/udacity/machine-learning/raw/756d918d21551d3c3668ec6ee680947b253d1214/projects/practice_projects/naive_bayes_tutorial/images/naivebayes.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ==> the Names Jill and Gary\n",
    "x1 .. xn ==> the words\n",
    "by bays - every word is independent\n",
    "\n",
    "To break this down, we have to compute the following posterior probabilities:\n",
    "\n",
    "P(J|F,I): Probability of Jill Stein saying the words Freedom and Immigration.\n",
    "            Using the formula and our knowledge of Bayes' theorem, we can compute this as follows: \n",
    "            P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I).\n",
    "            Here P(F,I) is the probability of the words 'freedom' and 'immigration' being said in a speech.\n",
    "\n",
    "P(G|F,I): Probability of Gary Johnson saying the words Freedom and Immigration.\n",
    "            Using the formula, we can compute this as follows:\n",
    "            P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words freedom and immigration being said are:  0.075\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text\n",
    "\n",
    "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text\n",
    "\n",
    "The third step is to add both of these probabilities and you will get P(F,I).\n",
    "'''\n",
    "p_j = 0.5\n",
    "p_j_f = 0.1\n",
    "p_j_i = 0.1\n",
    "p_j_text = p_j * p_j_f * p_j_i\n",
    "p_g = 0.5\n",
    "p_g_f = 0.7\n",
    "p_g_i = 0.2\n",
    "p_g_text = p_g * p_g_f * p_g_i\n",
    "# Compute P(F,I) and store in p_f_i\n",
    "p_f_i = p_j_text + p_g_text\n",
    "print('Probability of words freedom and immigration being said are: ', format(p_f_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the probability of P(J|F,I), that is the probability of Jill Stein saying the words Freedom and Immigration and P(G|F,I), that is the probability of Gary Johnson saying the words Freedom and Immigration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n",
      "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
    "and same for Gary\n",
    "'''\n",
    "p_j_fi = (p_j * p_j_f * p_j_i) / p_f_i\n",
    "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))\n",
    "\n",
    "'''\n",
    "Instructions:\n",
    "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
    "'''\n",
    "p_g_fi = (p_g * p_g_f * p_g_i) / p_f_i\n",
    "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compard the the 93.3% chance for Gary Johnson of the Libertarian party.\n",
    "\n",
    "Applying this to our problem of classifying messages as spam, the Naive Bayes algorithm looks at each word individually and not as associated entities with any kind of link between them. In the case of spam detectors, this usually works as there are certain red flag words which can almost guarantee its classification as spam, for example emails with words like 'viagra' are usually classified as spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally we do it with sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "\n",
    "We have loaded the training data into the variable 'training_data' and the testing data into the \n",
    "variable 'testing_data'.\n",
    "\n",
    "Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\n",
    "'naive_bayes'. You will be training the classifier using 'training_data' and y_train' from our split earlier. \n",
    "'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(training_data, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Now that our algorithm has been trained using the training data set we can now make some predictions on the test data\n",
    "stored in 'testing_data' using predict(). Save your predictions into the 'predictions' variable.\n",
    "'''\n",
    "predictions = MNB.predict(testing_data)\n",
    "# now finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "\n",
    "Accuracy measures how often the classifier makes the correct prediction. Itâ€™s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "Precision tells us what proportion of messages we classified as spam, actually were spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of\n",
    "\n",
    "[True Positives/(True Positives + False Positives)]\n",
    "\n",
    "Recall(sensitivity) tells us what proportion of messages that actually were spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n",
    "\n",
    "[True Positives/(True Positives + False Negatives)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.9869494290375204\n",
      "Precision score:  0.9666666666666667\n",
      "Recall score:  0.9354838709677419\n",
      "F1 score:  0.9508196721311476\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the accuracy, precision, recall and F1 scores of your model using your test data 'y_test' and the predictions\n",
    "you made earlier stored in the 'predictions' variable.\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: ', format(accuracy_score(Y_test, predictions)))\n",
    "print('Precision score: ', format(precision_score(Y_test, predictions)))\n",
    "print('Recall score: ', format(recall_score(Y_test, predictions)))\n",
    "print('F1 score: ', format(f1_score(Y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle. All in all, Naive Bayes' really is a gem of an algorithm!\n",
    "Congratulations! You have succesfully designed a model that can efficiently predict if an SMS message is spam or not!\n",
    "Thank you for learning with us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
